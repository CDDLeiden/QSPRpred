import itertools
import logging
import os
import random
import traceback
from concurrent.futures import ProcessPoolExecutor
from multiprocessing import Lock
from typing import Generator

import pandas as pd

from .settings.benchmark import BenchmarkSettings
from .replica import Replica

lock = Lock()


class BenchmarkRunner:
    """Class that runs benchmarking experiments as defined by
    `BenchmarkSettings`. It translates the settings into
    a list of `Replica` objects with its `iterReplicas` method and
    runs them in parallel. Each replica is processed by the `runReplica`
    method.

    The report from each replica is appended to a `resultsFile`, which
    is read and returned by the `run` method after the runners is finished
    with all replicas. All outputs generated by the replicas and the `BenchmarkSettings`
    used are saved in the `dataDir`.

    The random seed for each replica is determined in a pseudo-random way from
     `BenchmarkSettings.random_seed`. The `getSeedList` method is used to generate
     a list of seeds from this 'master' seed. There are some caveats to this method
     (see the docstring of `getSeedList`).

    Attributes:
        settings (BenchmarkSettings):
            Benchmark settings.
        nProc (int):
            Number of processes to use.
        resultsFile (str):
            Path to the results file.
        dataDir (str):
            Path to the directory to store data.

    """

    class ReplicaException(Exception):
        """Custom exception for errors in a replica.

        Attributes:
            replicaID (int):
                ID of the replica that caused the error.
            exception (Exception):
                Exception that was raised.
        """

        def __init__(self, replica_id: int, exception: Exception):
            """Initialize the exception.

            Args:
                replica_id (int):
                    ID of the replica that caused the error.
                exception (Exception):
                    Exception that was raised.
            """
            self.replicaID = replica_id
            self.exception = exception

    def __init__(
            self,
            settings: BenchmarkSettings,
            n_proc: int | None = None,
            data_dir: str = "./data",
            results_file: str = "./data/results.tsv"
    ):
        """Initialize the runner.

        Args:
            settings (BenchmarkSettings):
                Benchmark settings.
            n_proc (int, optional):
                Number of processes to use. Defaults to os.cpu_count().
            data_dir (str, optional):
                Path to the directory to store data. Defaults to "./data".
                If the directory does not exist, it will be created.
            results_file (str, optional):
                Path to the results file. Defaults to "./data/results.tsv".
        """
        self.settings = settings
        self.nProc = n_proc or os.cpu_count()
        self.resultsFile = results_file
        self.dataDir = data_dir
        os.makedirs(self.dataDir, exist_ok=True)

    @property
    def nRuns(self) -> int:
        """Returns the total number of benchmarking runs. This is the product
        of the number of replicas, data sources, descriptors, target properties,
        data preparation settings and models as defined in the `BenchmarkSettings`.

        Returns:
            int:
                Total number of benchmarking runs.
        """
        benchmark_settings = self.settings
        benchmark_settings.checkConsistency()
        ret = (benchmark_settings.n_replicas * len(benchmark_settings.data_sources)
               * len(benchmark_settings.descriptors) * len(
                    benchmark_settings.target_props)
               * len(benchmark_settings.prep_settings) * len(benchmark_settings.models)
               )
        if len(benchmark_settings.optimizers) > 0:
            ret *= len(benchmark_settings.optimizers)
        return ret

    def run(self, raise_errors=False) -> pd.DataFrame:
        """Runs the benchmarking experiments.

        Args:
            raise_errors (bool, optional):
                Whether to raise the first encountered `ReplicaException`
                and stop the benchmarking run. Defaults to `False`,
                in which case replicas that raise an exception are skipped
                and errors are logged.

        Returns:
            pd.DataFrame:
                Results from the benchmarking experiments.
        """
        logging.info(f"Saving settings to: {self.dataDir}/settings.json")
        self.settings.toFile(f"{self.dataDir}/settings.json")
        logging.info(f"Performing {self.nRuns} replica runs...")
        with ProcessPoolExecutor(max_workers=self.nProc) as executor:
            for result in executor.map(
                    self.runReplica,
                    self.iterReplicas()
            ):
                if isinstance(result, self.ReplicaException):
                    if raise_errors:
                        raise result.exception
                    else:
                        logging.error(
                            f"Error in replica {result.replicaID}: {result.exception}"
                        )
        logging.info("Finished all replica runs.")
        return pd.read_table(self.resultsFile)

    def getSeedList(self, seed: int | None = None) -> list[int]:
        """
        Get a list of seeds for the replicas from one 'master' seed.
        The list of seeds is generated by sampling from the range of
        possible seeds (0 to 2^32 - 1) with the given seed as the random
        seed for the random module. This means that the list of seeds
        will be the same for each run of the benchmarking experiment
        with the same 'master' seed. This is useful for reproducibility,
        but it also avoids recalculating replicas that were already calculated.

        Caveat: If the seed in `BenchmarkSettings.random_seed` is the same, but
        the number of replicas is different (i.e. the settings themselves change)
        then this code will still generate the same seeds for experiments that
        might not overlap with previous experiments. Therefore, take this into account
        when you already calculated some replicas, but decided to change your experiment
        settings.

        Args:
            seed (int, optional):
                'Master' seed. Defaults to `BenchmarkSettings.random_seed`.

        Returns:
            list[int]:
                list of seeds for the replicas
        """
        seed = seed or self.settings.random_seed
        random.seed(seed)
        return random.sample(range(2 ** 32 - 1), self.nRuns)

    def iterReplicas(self) -> Generator[Replica, None, None]:
        """Generator that yields `Replica` objects for each benchmarking run.
        This is done by iterating over the product of the data sources, descriptors,
        target properties, data preparation settings, models and optimizers as defined
        in the `BenchmarkSettings`. The random seed for each replica is determined
        in a pseudo-random way from `BenchmarkSettings.random_seed` via
        the `getSeedList` method.

        Yields:
            Generator[Replica, None, None]:
                `Replica` objects for each benchmarking run.
        """
        benchmark_settings = self.settings
        benchmark_settings.checkConsistency()
        indices = [x+1 for x in range(benchmark_settings.n_replicas)]
        optimizers = benchmark_settings.optimizers if len(benchmark_settings.optimizers) > 0 else [None]
        product = itertools.product(
            indices,
            [benchmark_settings.name],
            benchmark_settings.data_sources,
            benchmark_settings.descriptors,
            benchmark_settings.target_props,
            benchmark_settings.prep_settings,
            benchmark_settings.models,
            optimizers,
        )
        seeds = self.getSeedList(benchmark_settings.random_seed)
        for idx, settings in enumerate(product):
            yield Replica(
                *settings,
                random_seed=seeds[idx],
                assessors=benchmark_settings.assessors
            )

    def runReplica(self, replica: Replica):
        """Runs a single replica. This is executed in parallel by the `run` method.

        Args:
            replica (Replica):
                Replica to run.

        Returns:
            pd.DataFrame | ReplicaException:
                Results from the replica or a `ReplicaException` if an error occurred.
        """
        try:
            with lock:
                df_results = None
                if os.path.exists(self.resultsFile):
                    df_results = pd.read_table(self.resultsFile)
                if df_results is not None and df_results.ReplicaID.isin([replica.id]).any():
                    logging.warning(f"Skipping {replica.id}")
                    return
                replica.create_dataset(reload=False)
            replica.prep_dataset()
            replica.init_model()
            replica.run_assessment()
            with lock:
                df_report = replica.create_report()
                df_report.to_csv(
                    self.resultsFile,
                    sep="\t",
                    index=False,
                    mode="a",
                    header=not os.path.exists(self.resultsFile)
                )
                return df_report
        except Exception as e:
            traceback.print_exception(type(e), e, e.__traceback__)
            return self.ReplicaException(replica.id, e)
